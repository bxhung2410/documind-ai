import os
import time
import streamlit as st
from dotenv import load_dotenv

# LangChain components
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA

# Load environment variables from .env file
load_dotenv()

# --- Functions from previous script ---

def process_documents(document_path, db_directory):
    """
    Loads a single document, splits it, and creates a persisted vector store.
    """
    print(f"Loading document: {document_path}")
    loader = PyPDFLoader(document_path)
    documents = loader.load()

    print("Splitting document into chunks...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    texts = text_splitter.split_documents(documents)

    print("Creating embeddings and vector store...")
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

    # Create a new vector store from the documents
    vectordb = Chroma.from_documents(
        documents=texts,
        embedding=embeddings,
        persist_directory=db_directory
    )
    vectordb.persist()
    print("Vector store created and saved.")
    return vectordb

def get_qa_chain(db_directory):
    """
    Loads an existing vector store and creates a QA chain.
    """
    print("Loading existing vector store...")
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vectordb = Chroma(persist_directory=db_directory, embedding_function=embeddings)

    print("Creating the QA chain...")
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro", temperature=0, convert_system_message_to_human=True)
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectordb.as_retriever()
    )
    return qa_chain

# --- Streamlit Web App Interface ---

# Set page title
st.set_page_config(page_title="Tr·ª£ l√Ω T√†i li·ªáu AI")
st.title("üí¨ Tr·ª£ l√Ω T√†i li·ªáu AI")

# Sidebar for file upload
with st.sidebar:
    st.header("T√†i li·ªáu c·ªßa b·∫°n")
    uploaded_file = st.file_uploader("T·∫£i l√™n m·ªôt file PDF v√† nh·∫•n 'X·ª≠ l√Ω'", type="pdf")
    
    if st.button("X·ª≠ l√Ω"):
        if uploaded_file is not None:
            # Create a temporary directory for this session's data
            session_db_dir = f"db/{uploaded_file.file_id}"
            if not os.path.exists(session_db_dir):
                os.makedirs(session_db_dir)

            # Save the uploaded file temporarily
            filepath = os.path.join(session_db_dir, uploaded_file.name)
            with open(filepath, "wb") as f:
                f.write(uploaded_file.getbuffer())

            with st.spinner("ƒêang x·ª≠ l√Ω t√†i li·ªáu... (vi·ªác n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t)"):
                process_documents(filepath, session_db_dir)
            
            st.success("T√†i li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω! B√¢y gi·ªù b·∫°n c√≥ th·ªÉ h·ªèi.")
            st.session_state.db_dir = session_db_dir
            st.session_state.messages = [] # Clear previous messages
        else:
            st.warning("Vui l√≤ng t·∫£i l√™n m·ªôt file PDF.")

# Initialize or load chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Main chat interface
if "db_dir" in st.session_state:
    # Create the QA chain from the processed document's database
    qa_chain = get_qa_chain(st.session_state.db_dir)

    if prompt := st.chat_input("H√£y h·ªèi m·ªôt c√¢u v·ªÅ t√†i li·ªáu c·ªßa b·∫°n..."):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Get AI response
        with st.chat_message("assistant"):
            with st.spinner("AI ƒëang suy nghƒ©..."):
                try:
                    result = qa_chain.invoke(prompt)
                    response = result['result']
                    st.markdown(response)
                    # Add AI response to chat history
                    st.session_state.messages.append({"role": "assistant", "content": response})
                except Exception as e:
                    st.error(f"ƒê√£ x·∫£y ra l·ªói: {e}")
else:
    st.info("Ch√†o m·ª´ng! Vui l√≤ng t·∫£i l√™n m·ªôt t√†i li·ªáu PDF ·ªü thanh b√™n tr√°i ƒë·ªÉ b·∫Øt ƒë·∫ßu.")